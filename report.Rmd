---
title: "Data Science Specialisation: Capstone Milestone Report"
output: html_document
fig_caption: yes
author: Jo Stichbury
date: 13th June 2016
---

Goals for this assignment:

1. Demonstrate that you've downloaded the data and have successfully loaded it in.
2. Create a basic report of summary statistics about the data sets.
3. Report any interesting findings that you amassed so far.
4. Get feedback on your plans for creating a prediction algorithm and Shiny app.

The data were downloaded from https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip
I unzipped the file and concentrated on the en_US set only, which comprises 3 files. 

### Data analysis

Below is a table of statistics gathered from the 3 data files.

```{r echo=FALSE}
files<-c("en_US.blogs.txt","en_US.news.txt","en_US.twitter.txt")
size<-c(210160014, 205811889, 167105338)
lines<-c(899288,1010242,2360148)
df<-cbind(files, size, lines)

labels<-c("File name", "File size (bytes)", "Number of lines")
colnames(df)<-labels
df
```


The combined size of the three files is over 500 MB and too big in size for my desktop resources to handle. To explore the data further, I took chunks of each file by reading the files in line-by-line and flipping a biased coin to copy each line to a new file (probability of 1 in 200). The three new files were thus approximately 1/200th of the size of the originals (total corpus size was 2.8Mb).

To investigate the data, I used the quanteda package, which makes it straightforward to:  
* convert all text to lower case   
* remove numbers, separators, whitespace and punctuation   
* remove Twitter handles markers and hashtag markers   

I used the list from http://www.bannedwordlist.com for profanity filtering (77 words removed).

Below is a table of statistics from the three smaller (subset) files after they were loaded into quanteda using the textfile() method. Note that "tokens" are the number of words in each file (total words), while "types" are unique words.

```{r echo=FALSE, warning=FALSE, message=FALSE}
files<-c("en_US.blogs-0.005.txt","en_US.news-005.txt","en_US.twitter-005.txt")
size<-c(1075024, 1050861, 836218)
df<-cbind(files, size)

labels<-c("File name", "File size (bytes)")
colnames(df)<-labels
df

library(quanteda)
tf <- textfile("*.txt", cache=FALSE)
summary(corpus(tf))
```


I used the dfm() method to create document feature matrix objects for ngrams =1, 2 and 3.  From these, it's easy to list out the most popular unigrams, bigrams and trigrams in my sample corpus. The following word clouds show the top 75 of each, and the histograms below show an alternative view.

Word cloud for ngrams=1   
![](ngrams1-75words.png)  
Word cloud for ngrams=2  
![](ngrams2-75words.png)  
Word cloud for ngrams=3  
![](ngrams3-75words.png)  


The histograms show the distribution of popular words, which is that many n-grams appear rarely and only a small fraction appear frequently.
![Histogram for unigrams](75unigrams-barplot.png)
![Histogram for bigrams](75bigrams-barplot.png)
![Histogram for trigrams](75trigrams-barplot.png)



##Plans for creating the prediction algorithm and Shiny app
I will explore the frequencies of the n-grams to answer questions such as "How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language?" I also need to work through the modeling section in the Coursera notes, and consider size and speed of the model, given my resource limitations. I will then go ahead and build the prediction model, which is likely to be a second-order Markov model. I'm a little behind where I hoped to be by this stage, so hope to press ahead soon and catch up!   

Thanks for the review :)

##Code
Code for my exploratory data analysis is stored at https://github.com/stichbury/capstone-milestone




